#!/usr/bin/env python3
"""
Script SIMPLE pour tester l'Ã‰poque 30 sur vidÃ©os
Support: MP4, AVI, MOV, MKV, WMV, MPG, MPEG
Usage: python simple_video_test_epoch30.py
"""

import os
import cv2
import torch
import numpy as np
import torchvision.transforms as transforms
import torchvision.models.detection as detection_models
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import glob

# Configuration simple
EPOCH_30_MODEL = "output_stable_training/stable_model_epoch_30.pth"
VIDEOS_DIR = "videos"
OUTPUT_DIR = "videos_output"
CONFIDENCE = 0.5  # Seuil optimal dÃ©montrÃ©

# Classes du modÃ¨le (28 classes)
CLASSES_FR = [
    'Personne', 'Sac Ã  dos', 'Valise', 'Sac Ã  main', 'Cravate',
    'Parapluie', 'SÃ¨che-cheveux', 'Brosse Ã  dents', 'TÃ©lÃ©phone',
    'Ordinateur portable', 'Clavier', 'Souris', 'TÃ©lÃ©commande', 'TÃ©lÃ©vision',
    'Horloge', 'Micro-ondes', 'Bouteille', 'Tasse', 'Bol',
    'Couteau', 'CuillÃ¨re', 'Fourchette', 'Verre', 'RÃ©frigÃ©rateur',
    'Ciseaux', 'Livre', 'Vase', 'Chaise'
]

def load_epoch30_model():
    """Charge le modÃ¨le epoch 30"""
    print("ğŸ¤– Chargement du modÃ¨le Ã‰poque 30...")
    
    if not os.path.exists(EPOCH_30_MODEL):
        print(f"âŒ ModÃ¨le non trouvÃ©: {EPOCH_30_MODEL}")
        return None, None
    
    try:
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        model = detection_models.fasterrcnn_resnet50_fpn(weights=None)
        in_features = model.roi_heads.box_predictor.cls_score.in_features
        model.roi_heads.box_predictor = FastRCNNPredictor(in_features, 29)  # 28 + fond
        
        model.load_state_dict(torch.load(EPOCH_30_MODEL, map_location=device))
        model.to(device)
        model.eval()
        
        print(f"âœ… ModÃ¨le epoch 30 chargÃ© sur {device}")
        return model, device
    except Exception as e:
        print(f"âŒ Erreur: {e}")
        return None, None

def find_videos():
    """Trouve toutes les vidÃ©os dans le dossier videos"""
    if not os.path.exists(VIDEOS_DIR):
        print(f"âŒ CrÃ©ez le dossier '{VIDEOS_DIR}' et placez-y vos vidÃ©os")
        return []
    
    # Extensions vidÃ©o supportÃ©es (AVEC MPG/MPEG AJOUTÃ‰S)
    extensions = ['*.mp4', '*.avi', '*.mov', '*.mkv', '*.wmv', '*.mpg', '*.mpeg', '*.m4v']
    videos = []
    
    for ext in extensions:
        videos.extend(glob.glob(os.path.join(VIDEOS_DIR, ext)))
        videos.extend(glob.glob(os.path.join(VIDEOS_DIR, ext.upper())))
    
    if not videos:
        print(f"âŒ Aucune vidÃ©o trouvÃ©e dans '{VIDEOS_DIR}'")
        print("ğŸ¬ Formats supportÃ©s: .mp4, .avi, .mov, .mkv, .wmv, .mpg, .mpeg, .m4v")
        print("ğŸ’¡ Placez vos vidÃ©os dans le dossier 'videos/'")
        return []
    
    print(f"ğŸ“¹ {len(videos)} vidÃ©o(s) trouvÃ©e(s):")
    for i, video in enumerate(videos, 1):
        # Obtenir info sur la vidÃ©o
        try:
            cap = cv2.VideoCapture(video)
            if cap.isOpened():
                width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
                height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
                fps = int(cap.get(cv2.CAP_PROP_FPS))
                frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
                duration = frame_count / fps if fps > 0 else 0
                
                # Taille du fichier
                size_mb = os.path.getsize(video) / (1024 * 1024)
                
                print(f"  {i:2d}. {os.path.basename(video):<30} "
                      f"({width}x{height}, {fps}fps, {duration:.1f}s, {size_mb:.1f}MB)")
                cap.release()
            else:
                print(f"  {i:2d}. {os.path.basename(video):<30} (âŒ Erreur lecture)")
        except:
            print(f"  {i:2d}. {os.path.basename(video):<30} (â“ Info indisponible)")
    
    return videos

def select_video(videos):
    """SÃ©lectionne une vidÃ©o Ã  traiter"""
    if len(videos) == 1:
        print(f"ğŸ“¹ VidÃ©o sÃ©lectionnÃ©e automatiquement: {os.path.basename(videos[0])}")
        return videos[0]
    
    print(f"\nğŸ¯ SÃ‰LECTION DE LA VIDÃ‰O:")
    while True:
        try:
            choice = input(f"\nChoisissez une vidÃ©o (1-{len(videos)}) ou 'q' pour quitter: ").strip()
            if choice.lower() == 'q':
                return None
            
            idx = int(choice) - 1
            if 0 <= idx < len(videos):
                selected = videos[idx]
                print(f"âœ… SÃ©lectionnÃ©: {os.path.basename(selected)}")
                return selected
            else:
                print(f"âŒ Choisissez un nombre entre 1 et {len(videos)}")
        except ValueError:
            print("âŒ Entrez un nombre valide ou 'q' pour quitter")

def test_video_compatibility(video_path):
    """Test la compatibilitÃ© de la vidÃ©o"""
    print(f"\nğŸ” Test de compatibilitÃ©: {os.path.basename(video_path)}")
    
    try:
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            print("âŒ Impossible d'ouvrir la vidÃ©o")
            print("ğŸ’¡ Essayez de convertir avec: ffmpeg -i input.mpg output.mp4")
            return False
        
        # Lire quelques frames de test
        ret, frame = cap.read()
        if not ret:
            print("âŒ Impossible de lire les frames")
            cap.release()
            return False
        
        # VÃ©rifier les propriÃ©tÃ©s
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        fps = int(cap.get(cv2.CAP_PROP_FPS))
        
        print(f"âœ… VidÃ©o compatible:")
        print(f"   ğŸ“ RÃ©solution: {width}x{height}")
        print(f"   ğŸ¬ FPS: {fps}")
        print(f"   ğŸ–¼ï¸ Frame test: OK")
        
        cap.release()
        return True
        
    except Exception as e:
        print(f"âŒ Erreur test: {e}")
        return False

def process_video(model, device, video_path):
    """Traite une vidÃ©o avec dÃ©tection"""
    print(f"\nğŸ¬ TRAITEMENT DE: {os.path.basename(video_path)}")
    print("="*60)
    
    # Test de compatibilitÃ© d'abord
    if not test_video_compatibility(video_path):
        return False
    
    # Ouvrir la vidÃ©o
    cap = cv2.VideoCapture(video_path)
    
    # PropriÃ©tÃ©s vidÃ©o
    fps = int(cap.get(cv2.CAP_PROP_FPS))
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    
    print(f"ğŸ“Š PropriÃ©tÃ©s: {width}x{height} Ã  {fps} FPS")
    print(f"â±ï¸ DurÃ©e: {total_frames/fps:.1f} secondes ({total_frames} frames)")
    
    # Fichier de sortie
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # Garder l'extension originale dans le nom
    base_name = os.path.splitext(os.path.basename(video_path))[0]
    original_ext = os.path.splitext(video_path)[1]
    output_path = os.path.join(OUTPUT_DIR, f"detected_{base_name}{original_ext}")
    
    # Si c'est un MPG, convertir en MP4 pour la sortie
    if original_ext.lower() in ['.mpg', '.mpeg']:
        output_path = os.path.join(OUTPUT_DIR, f"detected_{base_name}.mp4")
        print(f"ğŸ’¡ Conversion {original_ext} â†’ .mp4 pour la sortie")
    
    # Writer pour sauvegarder
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
    
    # Transformation pour le modÃ¨le
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])
    
    # Couleurs alÃ©atoires pour les classes
    np.random.seed(42)
    colors = [(np.random.randint(50, 255), np.random.randint(50, 255), np.random.randint(50, 255)) 
              for _ in range(28)]
    
    frame_count = 0
    total_detections = 0
    processing_times = []
    
    print("ğŸš€ Traitement en cours...")
    print("ğŸ’¡ Traitement optimisÃ© pour performance maximale")
    
    try:
        import time
        start_time = time.time()
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            frame_count += 1
            frame_start_time = time.time()
            
            # PrÃ©traitement pour le modÃ¨le
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            frame_tensor = transform(frame_rgb).unsqueeze(0).to(device)
            
            # DÃ©tection
            with torch.no_grad():
                predictions = model(frame_tensor)
            
            # Extraire rÃ©sultats
            boxes = predictions[0]['boxes'].cpu().numpy()
            labels = predictions[0]['labels'].cpu().numpy()
            scores = predictions[0]['scores'].cpu().numpy()
            
            # Filtrer par confiance
            mask = scores > CONFIDENCE
            boxes = boxes[mask]
            labels = labels[mask]
            scores = scores[mask]
            
            frame_detections = len(boxes)
            total_detections += frame_detections
            
            # Dessiner les dÃ©tections
            for box, label, score in zip(boxes, labels, scores):
                x1, y1, x2, y2 = box.astype(int)
                class_idx = label - 1  # -1 car 0 est le fond
                
                if 0 <= class_idx < 28:
                    class_name = CLASSES_FR[class_idx]
                    color = colors[class_idx]
                    
                    # Rectangle de dÃ©tection
                    cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)
                    
                    # Texte avec nom et confiance
                    text = f"{class_name}: {score:.2f}"
                    (text_width, text_height), _ = cv2.getTextSize(
                        text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2
                    )
                    
                    # Fond du texte
                    cv2.rectangle(frame, (x1, y1-text_height-10), 
                                (x1+text_width, y1), color, -1)
                    
                    # Texte
                    cv2.putText(frame, text, (x1, y1-5), 
                              cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,255,255), 2)
            
            # Informations sur la frame
            info_text = f"Frame: {frame_count}/{total_frames} | Objets: {frame_detections} | Total: {total_detections}"
            cv2.putText(frame, info_text, (10, 30), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
            
            # Informations du modÃ¨le
            model_info = f"Modele: Epoque 30 | Format: {original_ext.upper()} | Confiance: {CONFIDENCE}"
            cv2.putText(frame, model_info, (10, height-10), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
            
            # Sauvegarder la frame
            writer.write(frame)
            
            # Mesurer performance
            frame_time = time.time() - frame_start_time
            processing_times.append(frame_time)
            
            # Affichage progression
            if frame_count % 50 == 0:
                progress = (frame_count / total_frames) * 100
                avg_time = np.mean(processing_times[-50:]) if processing_times else 0
                fps_actual = 1.0 / avg_time if avg_time > 0 else 0
                
                print(f"ğŸ“ˆ {progress:5.1f}% | {frame_count:4d}/{total_frames} frames | "
                      f"{total_detections:3d} dÃ©tections | {fps_actual:.1f} fps")
    
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ArrÃªt par Ctrl+C")
    
    finally:
        # Nettoyage
        cap.release()
        writer.release()
        
        # Statistiques finales
        end_time = time.time()
        total_time = end_time - start_time
        
        print(f"\nğŸ“Š RÃ‰SULTATS FINAUX:")
        print("="*50)
        print(f"   ğŸ¬ Frames traitÃ©es: {frame_count}/{total_frames}")
        print(f"   ğŸ” Total dÃ©tections: {total_detections}")
        print(f"   ğŸ“ˆ Moyenne par frame: {total_detections/frame_count:.1f}")
        print(f"   â±ï¸ Temps total: {total_time:.1f}s")
        print(f"   ğŸš€ Vitesse moyenne: {frame_count/total_time:.1f} fps")
        print(f"   ğŸ’¾ VidÃ©o sauvegardÃ©e: {output_path}")
        
        # VÃ©rifier si le fichier de sortie existe
        if os.path.exists(output_path):
            size_mb = os.path.getsize(output_path) / (1024 * 1024)
            print(f"   ğŸ“ Taille fichier: {size_mb:.1f} MB")
            print(f"   âœ… Format converti: {original_ext} â†’ {os.path.splitext(output_path)[1]}")
        
        return True

def main():
    """Fonction principale"""
    print("="*70)
    print("ğŸ¬ TEST VIDÃ‰O SIMPLE - MODÃˆLE Ã‰POQUE 30 (AVEC SUPPORT MPG)")
    print("="*70)
    print("ğŸ† Performance: F1=49.86% | PrÃ©cision=60.73%")
    print("ğŸ¯ Seuil optimal: 0.5")
    print("ğŸ“¹ Formats supportÃ©s: MP4, AVI, MOV, MKV, WMV, MPG, MPEG, M4V")
    
    # Charger le modÃ¨le
    model, device = load_epoch30_model()
    if model is None:
        return
    
    # Trouver les vidÃ©os
    videos = find_videos()
    if not videos:
        return
    
    # SÃ©lectionner une vidÃ©o
    selected_video = select_video(videos)
    if not selected_video:
        print("âŒ Aucune vidÃ©o sÃ©lectionnÃ©e")
        return
    
    # Traiter la vidÃ©o
    success = process_video(model, device, selected_video)
    
    if success:
        print(f"\nâœ… TRAITEMENT TERMINÃ‰ AVEC SUCCÃˆS!")
        print(f"ğŸ“ RÃ©sultat dans: {OUTPUT_DIR}/")
        print(f"ğŸ’¡ Conseil: Les fichiers MPG sont convertis en MP4 pour compatibilitÃ©")
    else:
        print(f"\nâŒ Erreur lors du traitement")
        print(f"ğŸ’¡ Si problÃ¨me avec MPG, essayez: ffmpeg -i video.mpg video.mp4")

if __name__ == "__main__":
    main()