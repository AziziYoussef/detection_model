#!/usr/bin/env python3
"""
Test COMPLET du mod√®le √âpoque 30 sur TOUT le dataset val2017
Usage: python test_epoch30_full_evaluation.py
"""

import os
import json
import torch
import numpy as np
from PIL import Image
import torchvision.transforms as transforms
import torchvision.models.detection as detection_models
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
from pycocotools.coco import COCO
from tqdm import tqdm
import time
import matplotlib.pyplot as plt

# Configuration
EPOCH_30_MODEL = "output_stable_training/stable_model_epoch_30.pth"
COCO_VAL_DIR = "coco_evaluation"
IMAGE_SIZE = (320, 320)

# Classes des mod√®les stable (28 classes)
STABLE_CLASSES = [
    'person', 'backpack', 'suitcase', 'handbag', 'tie',
    'umbrella', 'hair drier', 'toothbrush', 'cell phone',
    'laptop', 'keyboard', 'mouse', 'remote', 'tv',
    'clock', 'microwave', 'bottle', 'cup', 'bowl',
    'knife', 'spoon', 'fork', 'wine glass', 'refrigerator',
    'scissors', 'book', 'vase', 'chair'
]

CLASSES_FR = {
    'person': 'Personne', 'backpack': 'Sac √† dos', 'suitcase': 'Valise',
    'handbag': 'Sac √† main', 'tie': 'Cravate', 'hair drier': 'S√®che-cheveux',
    'toothbrush': 'Brosse √† dents', 'cell phone': 'T√©l√©phone',
    'laptop': 'Ordinateur portable', 'keyboard': 'Clavier', 'mouse': 'Souris',
    'remote': 'T√©l√©commande', 'tv': 'T√©l√©vision', 'bottle': 'Bouteille',
    'cup': 'Tasse', 'bowl': 'Bol', 'knife': 'Couteau', 'spoon': 'Cuill√®re',
    'fork': 'Fourchette', 'wine glass': 'Verre', 'scissors': 'Ciseaux',
    'book': 'Livre', 'clock': 'Horloge', 'umbrella': 'Parapluie',
    'vase': 'Vase', 'chair': 'Chaise', 'microwave': 'Micro-ondes',
    'refrigerator': 'R√©frig√©rateur'
}

def check_setup():
    """V√©rifie la configuration compl√®te"""
    print("üîç V√©rification de la configuration...")
    
    # V√©rifier le mod√®le epoch 30
    if not os.path.exists(EPOCH_30_MODEL):
        print(f"‚ùå Mod√®le epoch 30 manquant: {EPOCH_30_MODEL}")
        return False
    
    # V√©rifier les annotations val2017
    ann_file = os.path.join(COCO_VAL_DIR, 'annotations', 'instances_val2017.json')
    if not os.path.exists(ann_file):
        print(f"‚ùå Annotations manquantes: {ann_file}")
        return False
    
    # V√©rifier les images val2017
    img_dir = os.path.join(COCO_VAL_DIR, 'images', 'val2017')
    if not os.path.exists(img_dir):
        print(f"‚ùå Images manquantes: {img_dir}")
        return False
    
    # Compter les images
    import glob
    images = glob.glob(os.path.join(img_dir, "*.jpg"))
    
    print(f"‚úÖ Configuration compl√®te v√©rifi√©e:")
    print(f"   ü§ñ Mod√®le: {os.path.basename(EPOCH_30_MODEL)}")
    print(f"   üìä Annotations: instances_val2017.json")
    print(f"   üì∑ Images: {len(images)} images val2017")
    
    return True

def load_epoch30_model():
    """Charge le mod√®le epoch 30"""
    print("ü§ñ Chargement du mod√®le √âpoque 30...")
    
    try:
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        model = detection_models.fasterrcnn_resnet50_fpn(weights=None)
        in_features = model.roi_heads.box_predictor.cls_score.in_features
        model.roi_heads.box_predictor = FastRCNNPredictor(in_features, len(STABLE_CLASSES) + 1)
        
        model.load_state_dict(torch.load(EPOCH_30_MODEL, map_location=device))
        model.to(device)
        model.eval()
        
        print(f"‚úÖ Mod√®le epoch 30 charg√© sur {device}")
        return model, device
    except Exception as e:
        print(f"‚ùå Erreur chargement: {e}")
        return None, None

def load_val2017_data():
    """Charge toutes les donn√©es val2017"""
    print("üìä Chargement des donn√©es val2017...")
    
    ann_file = os.path.join(COCO_VAL_DIR, 'annotations', 'instances_val2017.json')
    img_dir = os.path.join(COCO_VAL_DIR, 'images', 'val2017')
    
    # Charger COCO
    coco = COCO(ann_file)
    
    # Trouver nos classes dans COCO
    available_class_ids = []
    missing_classes = []
    
    for class_name in STABLE_CLASSES:
        cat_ids = coco.getCatIds(catNms=[class_name])
        if cat_ids:
            available_class_ids.append(cat_ids[0])
        else:
            missing_classes.append(class_name)
    
    print(f"‚úÖ Classes trouv√©es: {len(available_class_ids)}/{len(STABLE_CLASSES)}")
    if missing_classes:
        print(f"‚ö†Ô∏è Classes manquantes: {missing_classes}")
    
    # Trouver TOUTES les images contenant nos classes
    all_img_ids = set()
    for class_id in available_class_ids:
        img_ids = coco.getImgIds(catIds=[class_id])
        all_img_ids.update(img_ids)
    
    all_img_ids = sorted(list(all_img_ids))
    
    # V√©rifier existence physique des images
    valid_images = []
    for img_id in all_img_ids:
        img_info = coco.loadImgs([img_id])[0]
        img_path = os.path.join(img_dir, img_info['file_name'])
        
        if os.path.exists(img_path):
            valid_images.append({
                'id': img_id,
                'path': img_path,
                'info': img_info
            })
    
    print(f"üì∑ Images valides: {len(valid_images)} sur {len(all_img_ids)}")
    
    return coco, valid_images, available_class_ids

def calculate_iou(box1, box2):
    """Calcule l'IoU entre deux bo√Ætes"""
    x1 = max(box1[0], box2[0])
    y1 = max(box1[1], box2[1])
    x2 = min(box1[2], box2[2])
    y2 = min(box1[3], box2[3])
    
    if x2 <= x1 or y2 <= y1:
        return 0.0
    
    intersection = (x2 - x1) * (y2 - y1)
    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])
    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])
    union = area1 + area2 - intersection
    
    return intersection / union if union > 0 else 0.0

def evaluate_on_image(model, image_path, coco, img_id, class_ids, device, confidence_threshold=0.3):
    """√âvalue le mod√®le sur une image"""
    
    # Pr√©traitement
    try:
        image = Image.open(image_path).convert('RGB')
        original_size = image.size
        
        # Redimensionner
        image_resized = image.resize(IMAGE_SIZE, Image.Resampling.BILINEAR)
        
        transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ])
        
        image_tensor = transform(image_resized).unsqueeze(0).to(device)
        
    except Exception as e:
        print(f"‚ùå Erreur pr√©traitement {image_path}: {e}")
        return None
    
    # Pr√©diction
    start_time = time.time()
    
    with torch.no_grad():
        predictions = model(image_tensor)
    
    inference_time = (time.time() - start_time) * 1000
    
    # Extraire pr√©dictions
    pred_boxes = predictions[0]['boxes'].cpu().numpy()
    pred_labels = predictions[0]['labels'].cpu().numpy()
    pred_scores = predictions[0]['scores'].cpu().numpy()
    
    # Filtrer par confiance
    mask = pred_scores > confidence_threshold
    pred_boxes = pred_boxes[mask]
    pred_labels = pred_labels[mask]
    pred_scores = pred_scores[mask]
    
    # Remettre √† l'√©chelle originale
    if len(pred_boxes) > 0:
        scale_x = original_size[0] / IMAGE_SIZE[0]
        scale_y = original_size[1] / IMAGE_SIZE[1]
        pred_boxes[:, [0, 2]] *= scale_x
        pred_boxes[:, [1, 3]] *= scale_y
    
    # Ground truth
    ann_ids = coco.getAnnIds(imgIds=[img_id])
    annotations = coco.loadAnns(ann_ids)
    
    gt_boxes = []
    gt_classes = []
    
    # Mapping des classes
    class_mapping = {}
    for i, class_name in enumerate(STABLE_CLASSES):
        cat_ids = coco.getCatIds(catNms=[class_name])
        if cat_ids:
            class_mapping[cat_ids[0]] = i + 1
    
    for ann in annotations:
        if ann['category_id'] in class_mapping:
            bbox = ann['bbox']  # [x, y, width, height]
            x1, y1, w, h = bbox
            x2, y2 = x1 + w, y1 + h
            
            gt_boxes.append([x1, y1, x2, y2])
            gt_classes.append(class_mapping[ann['category_id']])
    
    # Calcul des m√©triques
    num_gt = len(gt_boxes)
    num_pred = len(pred_boxes)
    
    if num_gt == 0 and num_pred == 0:
        return {
            'precision': 1.0, 'recall': 1.0, 'f1': 1.0,
            'tp': 0, 'fp': 0, 'fn': 0,
            'inference_time': inference_time,
            'avg_confidence': 1.0
        }
    
    if num_gt == 0:
        return {
            'precision': 0.0, 'recall': 1.0, 'f1': 0.0,
            'tp': 0, 'fp': num_pred, 'fn': 0,
            'inference_time': inference_time,
            'avg_confidence': float(np.mean(pred_scores)) if len(pred_scores) > 0 else 0.0
        }
    
    if num_pred == 0:
        return {
            'precision': 1.0, 'recall': 0.0, 'f1': 0.0,
            'tp': 0, 'fp': 0, 'fn': num_gt,
            'inference_time': inference_time,
            'avg_confidence': 0.0
        }
    
    # Matching avec IoU > 0.5
    tp = 0
    matched_gt = set()
    
    for pred_box in pred_boxes:
        best_iou = 0
        best_gt_idx = -1
        
        for gt_idx, gt_box in enumerate(gt_boxes):
            if gt_idx not in matched_gt:
                iou = calculate_iou(pred_box, gt_box)
                if iou > best_iou:
                    best_iou = iou
                    best_gt_idx = gt_idx
        
        if best_iou > 0.5:
            tp += 1
            matched_gt.add(best_gt_idx)
    
    fp = num_pred - tp
    fn = num_gt - tp
    
    precision = tp / num_pred if num_pred > 0 else 0
    recall = tp / num_gt if num_gt > 0 else 0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
    
    return {
        'precision': float(precision),
        'recall': float(recall),
        'f1': float(f1),
        'tp': int(tp),
        'fp': int(fp),
        'fn': int(fn),
        'inference_time': inference_time,
        'avg_confidence': float(np.mean(pred_scores)) if len(pred_scores) > 0 else 0.0
    }

def run_full_evaluation():
    """Lance l'√©valuation compl√®te"""
    print("="*80)
    print("üèÜ √âVALUATION COMPL√àTE - MOD√àLE √âPOQUE 30 SUR TOUT VAL2017")
    print("="*80)
    
    # V√©rifications
    if not check_setup():
        return
    
    # Charger le mod√®le
    model, device = load_epoch30_model()
    if model is None:
        return
    
    # Charger les donn√©es
    coco, valid_images, class_ids = load_val2017_data()
    if not valid_images:
        print("‚ùå Aucune image valide trouv√©e")
        return
    
    print(f"\nüöÄ D√âMARRAGE DE L'√âVALUATION COMPL√àTE")
    print(f"üìä Total images √† traiter: {len(valid_images)}")
    print(f"ü§ñ Mod√®le: √âpoque 30")
    print(f"üéØ Classes: {len(STABLE_CLASSES)}")
    
    # Seuil de confiance
    conf_thresh = input(f"\nSeuil de confiance (d√©faut 0.3): ").strip()
    try:
        confidence_threshold = float(conf_thresh) if conf_thresh else 0.3
    except ValueError:
        confidence_threshold = 0.3
    
    print(f"üéöÔ∏è Seuil de confiance: {confidence_threshold}")
    
    confirmation = input(f"\n‚ö†Ô∏è Cela va traiter {len(valid_images)} images (peut prendre du temps). Continuer? (y/n): ")
    if confirmation.lower() != 'y':
        print("‚ùå √âvaluation annul√©e")
        return
    
    # √âvaluation compl√®te
    print(f"\n‚è∞ D√©but de l'√©valuation... (estim√©: {len(valid_images)*0.12/60:.1f} minutes)")
    
    all_results = []
    failed_images = 0
    total_inference_time = 0
    
    # Barre de progression
    pbar = tqdm(valid_images, desc="√âvaluation √âpoque 30")
    
    for img_data in pbar:
        try:
            result = evaluate_on_image(
                model, img_data['path'], coco, img_data['id'], 
                class_ids, device, confidence_threshold
            )
            
            if result is not None:
                all_results.append(result)
                total_inference_time += result['inference_time']
                
                # Mise √† jour barre de progression
                if len(all_results) % 100 == 0:
                    current_f1 = np.mean([r['f1'] for r in all_results])
                    pbar.set_postfix({
                        'F1': f"{current_f1:.3f}",
                        'Images': len(all_results),
                        '√âchecs': failed_images
                    })
            else:
                failed_images += 1
                
        except Exception as e:
            failed_images += 1
            continue
    
    pbar.close()
    
    # Calcul des r√©sultats finaux
    if not all_results:
        print("‚ùå Aucun r√©sultat valide obtenu")
        return
    
    print(f"\nüìä CALCUL DES R√âSULTATS FINAUX...")
    
    # Moyennes globales
    avg_precision = float(np.mean([r['precision'] for r in all_results]))
    avg_recall = float(np.mean([r['recall'] for r in all_results]))
    avg_f1 = float(np.mean([r['f1'] for r in all_results]))
    avg_confidence = float(np.mean([r['avg_confidence'] for r in all_results]))
    avg_inference_time = float(total_inference_time / len(all_results))
    
    # Totaux
    total_tp = int(sum([r['tp'] for r in all_results]))
    total_fp = int(sum([r['fp'] for r in all_results]))
    total_fn = int(sum([r['fn'] for r in all_results]))
    
    # Affichage des r√©sultats
    display_final_results(
        avg_precision, avg_recall, avg_f1, avg_confidence, avg_inference_time,
        total_tp, total_fp, total_fn, len(all_results), failed_images,
        confidence_threshold
    )
    
    # Sauvegarde
    save_complete_results(all_results, {
        'precision': avg_precision,
        'recall': avg_recall,
        'f1': avg_f1,
        'confidence': avg_confidence,
        'inference_time': avg_inference_time,
        'tp': total_tp,
        'fp': total_fp,
        'fn': total_fn,
        'total_images': len(all_results),
        'failed_images': failed_images,
        'confidence_threshold': confidence_threshold
    })

def display_final_results(precision, recall, f1, confidence, inference_time, 
                         tp, fp, fn, total_images, failed_images, threshold):
    """Affiche les r√©sultats finaux"""
    print(f"\n{'='*80}")
    print("üèÜ R√âSULTATS FINAUX - √âPOQUE 30 SUR VAL2017 COMPLET")
    print(f"{'='*80}")
    
    print(f"üìä M√âTRIQUES PRINCIPALES:")
    print(f"   üéØ Pr√©cision:     {precision:.4f} ({precision*100:.2f}%)")
    print(f"   üîç Rappel:        {recall:.4f} ({recall*100:.2f}%)")
    print(f"   ‚öñÔ∏è F1-Score:      {f1:.4f} ({f1*100:.2f}%)")
    print(f"   üí™ Confiance:     {confidence:.4f} ({confidence*100:.2f}%)")
    
    print(f"\n‚ö° PERFORMANCE:")
    print(f"   ‚è±Ô∏è Temps moyen:    {inference_time:.2f}ms par image")
    print(f"   üñ•Ô∏è Vitesse:       {1000/inference_time:.1f} images/seconde")
    
    print(f"\nüìà D√âTAILS:")
    print(f"   ‚úÖ Vrais Positifs:  {tp}")
    print(f"   ‚ùå Faux Positifs:   {fp}")
    print(f"   ‚ùå Faux N√©gatifs:   {fn}")
    print(f"   üì∑ Images trait√©es: {total_images}")
    print(f"   üí• √âchecs:          {failed_images}")
    print(f"   üéöÔ∏è Seuil:          {threshold}")
    
    print(f"\nüéØ √âVALUATION CONTEXTUELLE:")
    
    if f1 >= 0.50:
        evaluation = "üèÜ EXCELLENT - Pr√™t pour production"
    elif f1 >= 0.40:
        evaluation = "‚úÖ BON - Performance satisfaisante"
    elif f1 >= 0.30:
        evaluation = "‚ö†Ô∏è CORRECT - Am√©liorations possibles"
    else:
        evaluation = "‚ùå FAIBLE - N√©cessite optimisation"
    
    print(f"   {evaluation}")
    print(f"   üìä F1-Score de {f1:.1%} sur {total_images} images")
    
    # Comparaison avec standards
    print(f"\nüìö COMPARAISON AVEC STANDARDS:")
    standards = {
        "D√©butant": 0.35,
        "Bon": 0.55, 
        "Tr√®s bon": 0.72,
        "Excellent": 0.85
    }
    
    for level, score in standards.items():
        status = "‚úÖ" if f1 >= score else "‚ùå"
        print(f"   {status} {level:<12} (F1 ‚â• {score:.2f})")

def save_complete_results(all_results, summary):
    """Sauvegarde les r√©sultats complets"""
    output_dir = "evaluation_results"
    os.makedirs(output_dir, exist_ok=True)
    
    # Sauvegarder r√©sultats d√©taill√©s
    detailed_file = os.path.join(output_dir, "epoch30_full_evaluation_detailed.json")
    with open(detailed_file, 'w') as f:
        json.dump(all_results, f, indent=2)
    
    # Sauvegarder r√©sum√©
    summary_file = os.path.join(output_dir, "epoch30_full_evaluation_summary.json")
    with open(summary_file, 'w') as f:
        json.dump(summary, f, indent=2)
    
    # Cr√©er graphique de performance
    create_performance_chart(all_results, summary)
    
    print(f"\nüíæ R√âSULTATS SAUVEGARD√âS:")
    print(f"   üìÑ D√©taill√©: {detailed_file}")
    print(f"   üìã R√©sum√©:   {summary_file}")
    print(f"   üìä Graphique: {output_dir}/epoch30_performance_chart.png")

def create_performance_chart(all_results, summary):
    """Cr√©e un graphique de performance"""
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
    
    # Distribution F1-Score
    f1_scores = [r['f1'] for r in all_results]
    ax1.hist(f1_scores, bins=50, alpha=0.7, color='skyblue', edgecolor='black')
    ax1.axvline(summary['f1'], color='red', linestyle='--', linewidth=2, label=f'Moyenne: {summary["f1"]:.3f}')
    ax1.set_xlabel('F1-Score')
    ax1.set_ylabel('Nombre d\'images')
    ax1.set_title('Distribution des F1-Scores')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # Pr√©cision vs Rappel
    precisions = [r['precision'] for r in all_results]
    recalls = [r['recall'] for r in all_results]
    ax2.scatter(precisions, recalls, alpha=0.5, s=10)
    ax2.set_xlabel('Pr√©cision')
    ax2.set_ylabel('Rappel')
    ax2.set_title('Pr√©cision vs Rappel')
    ax2.grid(True, alpha=0.3)
    
    # Temps d'inf√©rence
    inference_times = [r['inference_time'] for r in all_results]
    ax3.hist(inference_times, bins=50, alpha=0.7, color='lightgreen', edgecolor='black')
    ax3.axvline(summary['inference_time'], color='red', linestyle='--', linewidth=2, 
                label=f'Moyenne: {summary["inference_time"]:.1f}ms')
    ax3.set_xlabel('Temps d\'inf√©rence (ms)')
    ax3.set_ylabel('Nombre d\'images')
    ax3.set_title('Distribution des Temps d\'Inf√©rence')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # R√©sum√© des m√©triques
    metrics = ['Pr√©cision', 'Rappel', 'F1-Score', 'Confiance']
    values = [summary['precision'], summary['recall'], summary['f1'], summary['confidence']]
    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']
    
    bars = ax4.bar(metrics, values, color=colors, alpha=0.8)
    ax4.set_ylabel('Score')
    ax4.set_title('M√©triques Finales - √âpoque 30')
    ax4.set_ylim(0, 1)
    
    # Ajouter valeurs sur les barres
    for bar, value in zip(bars, values):
        ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                f'{value:.3f}', ha='center', va='bottom', fontweight='bold')
    
    ax4.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('evaluation_results/epoch30_performance_chart.png', dpi=300, bbox_inches='tight')
    plt.show()

def main():
    """Fonction principale"""
    print("üèÜ √âVALUATION COMPL√àTE - √âPOQUE 30")
    print("="*50)
    
    print("Ce script va √©valuer le mod√®le √âpoque 30 sur TOUT le dataset val2017")
    print("‚ö†Ô∏è Cela peut prendre du temps (plusieurs minutes √† heures selon votre GPU)")
    
    choice = input("\nLancer l'√©valuation compl√®te? (y/n): ").lower().strip()
    
    if choice == 'y':
        run_full_evaluation()
    else:
        print("‚ùå √âvaluation annul√©e")

if __name__ == "__main__":
    main()
