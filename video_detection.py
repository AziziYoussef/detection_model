#!/usr/bin/env python3
"""
Script rapide pour dÃ©tecter les objets perdus dans une vidÃ©o
Utilise le modÃ¨le epoch 9 (champion)
"""

import cv2
import torch
import torchvision.transforms as transforms
import torchvision.models.detection as detection_models
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import os

# Classes dÃ©tectables (28 classes rÃ©elles du modÃ¨le epoch 9)
CLASSES_FR = {
    'backpack': 'Sac Ã  dos',
    'suitcase': 'Valise', 
    'handbag': 'Sac Ã  main',
    'tie': 'Cravate',
    'hair drier': 'SÃ¨che-cheveux',
    'toothbrush': 'Brosse Ã  dents',
    'cell phone': 'TÃ©lÃ©phone',
    'laptop': 'Ordinateur portable',
    'keyboard': 'Clavier',
    'mouse': 'Souris',
    'remote': 'TÃ©lÃ©commande',
    'tv': 'TÃ©lÃ©vision',
    'bottle': 'Bouteille',
    'cup': 'Tasse',
    'bowl': 'Bol',
    'knife': 'Couteau',
    'spoon': 'CuillÃ¨re',
    'fork': 'Fourchette',
    'wine glass': 'Verre',
    'scissors': 'Ciseaux',
    'book': 'Livre',
    'clock': 'Horloge',
    'umbrella': 'Parapluie',
    'vase': 'Vase',
    'potted plant': 'Plante',
    'bicycle': 'VÃ©lo',
    'skateboard': 'Skateboard',
    'sports ball': 'Ballon'
}

CLASSES = list(CLASSES_FR.keys())

def load_model():
    """Charge le modÃ¨le epoch 9"""
    model_path = "output_extended_30/extended_model_epoch_9.pth"
    
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"ModÃ¨le non trouvÃ©: {model_path}")
    
    # CrÃ©er l'architecture
    model = detection_models.fasterrcnn_resnet50_fpn(weights=None)  # Nouveau paramÃ¨tre
    in_features = model.roi_heads.box_predictor.cls_score.in_features
    
    # CORRECTION: Utiliser 28 classes (nombre rÃ©el dans le modÃ¨le sauvegardÃ©)
    actual_num_classes = 28
    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, actual_num_classes + 1)
    
    # Charger les poids
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.to(device)
    model.eval()
    
    print(f"âœ… ModÃ¨le epoch 9 chargÃ© sur {device} avec {actual_num_classes} classes")
    return model, device

def detect_objects_in_video(video_path, output_path=None, confidence=0.5):
    """
    DÃ©tecte les objets dans une vidÃ©o
    
    Args:
        video_path (str): Chemin vers la vidÃ©o
        output_path (str): Chemin de sortie (optionnel)
        confidence (float): Seuil de confiance
    """
    print(f"ðŸŽ¬ Ouverture de la vidÃ©o: {video_path}")
    
    # Charger le modÃ¨le
    model, device = load_model()
    
    # Transformation
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])
    
    # Ouvrir la vidÃ©o
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        raise ValueError(f"Impossible d'ouvrir: {video_path}")
    
    # PropriÃ©tÃ©s vidÃ©o
    fps = int(cap.get(cv2.CAP_PROP_FPS))
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    
    print(f"ðŸ“Š {width}x{height} Ã  {fps} FPS, {total_frames} frames")
    
    # Writer pour sauvegarde
    writer = None
    if output_path:
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
        print(f"ðŸ’¾ Sauvegarde vers: {output_path}")
    
    # Couleurs pour les classes
    np.random.seed(42)
    colors = [(np.random.randint(50, 255), np.random.randint(50, 255), np.random.randint(50, 255)) 
              for _ in range(len(CLASSES))]
    
    frame_count = 0
    total_detections = 0
    
    print("ðŸš€ DÃ©but du traitement...")
    print("Appuyez sur 'q' pour quitter")
    
    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            frame_count += 1
            
            # PrÃ©traitement
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            frame_tensor = transform(frame_rgb).unsqueeze(0).to(device)
            
            # DÃ©tection
            with torch.no_grad():
                predictions = model(frame_tensor)
            
            # Traitement des rÃ©sultats
            boxes = predictions[0]['boxes'].cpu().numpy()
            labels = predictions[0]['labels'].cpu().numpy()
            scores = predictions[0]['scores'].cpu().numpy()
            
            # Filtrer par confiance
            mask = scores > confidence
            boxes = boxes[mask]
            labels = labels[mask]
            scores = scores[mask]
            
            frame_detections = len(boxes)
            total_detections += frame_detections
            
            # Annoter la frame
            for box, label, score in zip(boxes, labels, scores):
                x1, y1, x2, y2 = box.astype(int)
                class_idx = label - 1
                
                if 0 <= class_idx < len(CLASSES):
                    class_name = CLASSES[class_idx]
                    class_name_fr = CLASSES_FR[class_name]
                    color = colors[class_idx]
                    
                    # Rectangle
                    cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)
                    
                    # Texte
                    text = f"{class_name_fr}: {score:.2f}"
                    (text_width, text_height), _ = cv2.getTextSize(
                        text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2
                    )
                    
                    # Fond du texte
                    cv2.rectangle(frame, (x1, y1-text_height-10), (x1+text_width, y1), color, -1)
                    cv2.putText(frame, text, (x1, y1-5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,255,255), 2)
            
            # Informations sur la frame
            info = f"Frame: {frame_count}/{total_frames} | Objets: {frame_detections}"
            cv2.putText(frame, info, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
            
            # Affichage
            cv2.imshow('DÃ©tection Objets Perdus - Epoch 9', frame)
            
            # Sauvegarde
            if writer:
                writer.write(frame)
            
            # ContrÃ´le (quitter avec 'q')
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
            
            # Affichage progression (toutes les 30 frames)
            if frame_count % 30 == 0:
                progress = (frame_count / total_frames) * 100
                print(f"ðŸ“ˆ Progression: {progress:.1f}% - {frame_detections} objets dÃ©tectÃ©s")
    
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ArrÃªt demandÃ© par l'utilisateur")
    
    finally:
        # Nettoyage
        cap.release()
        if writer:
            writer.release()
        cv2.destroyAllWindows()
        
        # Statistiques finales
        print("\nðŸ“Š RÃ‰SULTATS FINAUX:")
        print(f"   ðŸŽ¬ Frames traitÃ©es: {frame_count}")
        print(f"   ðŸ” Total dÃ©tections: {total_detections}")
        print(f"   ðŸ“ˆ Moyenne par frame: {total_detections/frame_count:.1f}")

def detect_webcam():
    """DÃ©tection en temps rÃ©el sur webcam"""
    print("ðŸ“¹ DÃ©marrage webcam...")
    
    model, device = load_model()
    
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])
    
    cap = cv2.VideoCapture(0)
    if not cap.isOpened():
        raise ValueError("Impossible d'ouvrir la webcam")
    
    np.random.seed(42)
    colors = [(np.random.randint(50, 255), np.random.randint(50, 255), np.random.randint(50, 255)) 
              for _ in range(len(CLASSES))]
    
    print("ðŸš€ Webcam active - Appuyez sur 'q' pour quitter")
    
    frame_count = 0
    
    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            frame_count += 1
            
            # DÃ©tection (mÃªme logique que pour vidÃ©o)
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            frame_tensor = transform(frame_rgb).unsqueeze(0).to(device)
            
            with torch.no_grad():
                predictions = model(frame_tensor)
            
            boxes = predictions[0]['boxes'].cpu().numpy()
            labels = predictions[0]['labels'].cpu().numpy() 
            scores = predictions[0]['scores'].cpu().numpy()
            
            mask = scores > 0.5
            boxes = boxes[mask]
            labels = labels[mask]
            scores = scores[mask]
            
            # Annotation
            for box, label, score in zip(boxes, labels, scores):
                x1, y1, x2, y2 = box.astype(int)
                class_idx = label - 1
                
                if 0 <= class_idx < len(CLASSES):
                    class_name = CLASSES[class_idx]
                    class_name_fr = CLASSES_FR[class_name]
                    color = colors[class_idx]
                    
                    cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)
                    text = f"{class_name_fr}: {score:.2f}"
                    cv2.putText(frame, text, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)
            
            # Informations
            cv2.putText(frame, f"Webcam - Frame: {frame_count}", (10, 30), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
            cv2.putText(frame, "Appuyez sur 'q' pour quitter", (10, frame.shape[0]-10), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
            
            cv2.imshow('Webcam - Objets Perdus', frame)
            
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
    
    finally:
        cap.release()
        cv2.destroyAllWindows()
        print(f"ðŸ“Š Frames webcam traitÃ©es: {frame_count}")

if __name__ == "__main__":
    print("="*60)
    print("ðŸ† DÃ‰TECTION RAPIDE - MODÃˆLE EPOCH 9 (CHAMPION)")
    print("="*60)
    
    mode = input("Mode (1=VidÃ©o, 2=Webcam): ").strip()
    
    if mode == "1":
        video_file = input("Nom du fichier vidÃ©o: ").strip()
        if not video_file:
            video_file = "test.mp4"
        
        output_file = f"detected_{video_file}"
        
        try:
            detect_objects_in_video(video_file, output_file)
            print(f"âœ… VidÃ©o traitÃ©e et sauvegardÃ©e: {output_file}")
        except Exception as e:
            print(f"âŒ Erreur: {e}")
    
    elif mode == "2":
        try:
            detect_webcam()
        except Exception as e:
            print(f"âŒ Erreur webcam: {e}")
    
    else:
        print("âŒ Mode invalide")