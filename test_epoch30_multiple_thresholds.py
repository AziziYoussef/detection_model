#!/usr/bin/env python3
"""
Test COMPLET du mod√®le √âpoque 30 avec SEUILS MULTIPLES (0.5 √† 0.7)
Usage: python test_epoch30_thresholds_comparison.py
"""

import os
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
import json
import torch
import numpy as np
from PIL import Image
import torchvision.transforms as transforms
import torchvision.models.detection as detection_models
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
from pycocotools.coco import COCO
from tqdm import tqdm
import time
import matplotlib.pyplot as plt

# Configuration
EPOCH_30_MODEL = "output_stable_training/stable_model_epoch_30.pth"
COCO_VAL_DIR = "coco_evaluation"
IMAGE_SIZE = (320, 320)

# SEUILS √Ä TESTER (de 0.5 √† 0.7)
CONFIDENCE_THRESHOLDS = [0.5, 0.55, 0.6, 0.65, 0.7]

# Classes des mod√®les stable (28 classes)
STABLE_CLASSES = [
    'person', 'backpack', 'suitcase', 'handbag', 'tie',
    'umbrella', 'hair drier', 'toothbrush', 'cell phone',
    'laptop', 'keyboard', 'mouse', 'remote', 'tv',
    'clock', 'microwave', 'bottle', 'cup', 'bowl',
    'knife', 'spoon', 'fork', 'wine glass', 'refrigerator',
    'scissors', 'book', 'vase', 'chair'
]

def check_setup():
    """V√©rifie la configuration compl√®te"""
    print("üîç V√©rification de la configuration...")
    
    # V√©rifier le mod√®le epoch 30
    if not os.path.exists(EPOCH_30_MODEL):
        print(f"‚ùå Mod√®le epoch 30 manquant: {EPOCH_30_MODEL}")
        return False
    
    # V√©rifier les annotations val2017
    ann_file = os.path.join(COCO_VAL_DIR, 'annotations', 'instances_val2017.json')
    if not os.path.exists(ann_file):
        print(f"‚ùå Annotations manquantes: {ann_file}")
        return False
    
    # V√©rifier les images val2017
    img_dir = os.path.join(COCO_VAL_DIR, 'images', 'val2017')
    if not os.path.exists(img_dir):
        print(f"‚ùå Images manquantes: {img_dir}")
        return False
    
    # Compter les images
    import glob
    images = glob.glob(os.path.join(img_dir, "*.jpg"))
    
    print(f"‚úÖ Configuration compl√®te v√©rifi√©e:")
    print(f"   ü§ñ Mod√®le: {os.path.basename(EPOCH_30_MODEL)}")
    print(f"   üìä Annotations: instances_val2017.json")
    print(f"   üì∑ Images: {len(images)} images val2017")
    print(f"   üéØ Seuils √† tester: {CONFIDENCE_THRESHOLDS}")
    
    return True

def load_epoch30_model():
    """Charge le mod√®le epoch 30"""
    print("ü§ñ Chargement du mod√®le √âpoque 30...")
    
    try:
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        model = detection_models.fasterrcnn_resnet50_fpn(weights=None)
        in_features = model.roi_heads.box_predictor.cls_score.in_features
        model.roi_heads.box_predictor = FastRCNNPredictor(in_features, len(STABLE_CLASSES) + 1)
        
        model.load_state_dict(torch.load(EPOCH_30_MODEL, map_location=device))
        model.to(device)
        model.eval()
        
        print(f"‚úÖ Mod√®le epoch 30 charg√© sur {device}")
        return model, device
    except Exception as e:
        print(f"‚ùå Erreur chargement: {e}")
        return None, None

def load_val2017_data():
    """Charge toutes les donn√©es val2017"""
    print("üìä Chargement des donn√©es val2017...")
    
    ann_file = os.path.join(COCO_VAL_DIR, 'annotations', 'instances_val2017.json')
    img_dir = os.path.join(COCO_VAL_DIR, 'images', 'val2017')
    
    # Charger COCO
    coco = COCO(ann_file)
    
    # Trouver nos classes dans COCO
    available_class_ids = []
    missing_classes = []
    
    for class_name in STABLE_CLASSES:
        cat_ids = coco.getCatIds(catNms=[class_name])
        if cat_ids:
            available_class_ids.append(cat_ids[0])
        else:
            missing_classes.append(class_name)
    
    print(f"‚úÖ Classes trouv√©es: {len(available_class_ids)}/{len(STABLE_CLASSES)}")
    if missing_classes:
        print(f"‚ö†Ô∏è Classes manquantes: {missing_classes}")
    
    # Trouver TOUTES les images contenant nos classes
    all_img_ids = set()
    for class_id in available_class_ids:
        img_ids = coco.getImgIds(catIds=[class_id])
        all_img_ids.update(img_ids)
    
    all_img_ids = sorted(list(all_img_ids))
    
    # V√©rifier existence physique des images
    valid_images = []
    for img_id in all_img_ids:
        img_info = coco.loadImgs([img_id])[0]
        img_path = os.path.join(img_dir, img_info['file_name'])
        
        if os.path.exists(img_path):
            valid_images.append({
                'id': img_id,
                'path': img_path,
                'info': img_info
            })
    
    print(f"üì∑ Images valides: {len(valid_images)} sur {len(all_img_ids)}")
    
    return coco, valid_images, available_class_ids

def calculate_iou(box1, box2):
    """Calcule l'IoU entre deux bo√Ætes"""
    x1 = max(box1[0], box2[0])
    y1 = max(box1[1], box2[1])
    x2 = min(box1[2], box2[2])
    y2 = min(box1[3], box2[3])
    
    if x2 <= x1 or y2 <= y1:
        return 0.0
    
    intersection = (x2 - x1) * (y2 - y1)
    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])
    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])
    union = area1 + area2 - intersection
    
    return intersection / union if union > 0 else 0.0

def evaluate_on_image_all_thresholds(model, image_path, coco, img_id, class_ids, device):
    """√âvalue le mod√®le sur une image avec TOUS les seuils"""
    
    # Pr√©traitement (une seule fois)
    try:
        image = Image.open(image_path).convert('RGB')
        original_size = image.size
        
        # Redimensionner
        image_resized = image.resize(IMAGE_SIZE, Image.Resampling.BILINEAR)
        
        transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ])
        
        image_tensor = transform(image_resized).unsqueeze(0).to(device)
        
    except Exception as e:
        return None
    
    # Pr√©diction (une seule fois)
    start_time = time.time()
    
    with torch.no_grad():
        predictions = model(image_tensor)
    
    inference_time = (time.time() - start_time) * 1000
    
    # Extraire pr√©dictions BRUTES
    pred_boxes = predictions[0]['boxes'].cpu().numpy()
    pred_labels = predictions[0]['labels'].cpu().numpy()
    pred_scores = predictions[0]['scores'].cpu().numpy()
    
    # Remettre √† l'√©chelle originale
    if len(pred_boxes) > 0:
        scale_x = original_size[0] / IMAGE_SIZE[0]
        scale_y = original_size[1] / IMAGE_SIZE[1]
        pred_boxes[:, [0, 2]] *= scale_x
        pred_boxes[:, [1, 3]] *= scale_y
    
    # Ground truth (une seule fois)
    ann_ids = coco.getAnnIds(imgIds=[img_id])
    annotations = coco.loadAnns(ann_ids)
    
    gt_boxes = []
    gt_classes = []
    
    # Mapping des classes
    class_mapping = {}
    for i, class_name in enumerate(STABLE_CLASSES):
        cat_ids = coco.getCatIds(catNms=[class_name])
        if cat_ids:
            class_mapping[cat_ids[0]] = i + 1
    
    for ann in annotations:
        if ann['category_id'] in class_mapping:
            bbox = ann['bbox']  # [x, y, width, height]
            x1, y1, w, h = bbox
            x2, y2 = x1 + w, y1 + h
            
            gt_boxes.append([x1, y1, x2, y2])
            gt_classes.append(class_mapping[ann['category_id']])
    
    # √âVALUER POUR CHAQUE SEUIL
    results_by_threshold = {}
    
    for threshold in CONFIDENCE_THRESHOLDS:
        # Filtrer par confiance pour ce seuil
        mask = pred_scores > threshold
        filtered_boxes = pred_boxes[mask]
        filtered_labels = pred_labels[mask]
        filtered_scores = pred_scores[mask]
        
        # Calcul des m√©triques pour ce seuil
        num_gt = len(gt_boxes)
        num_pred = len(filtered_boxes)
        
        if num_gt == 0 and num_pred == 0:
            result = {
                'precision': 1.0, 'recall': 1.0, 'f1': 1.0,
                'tp': 0, 'fp': 0, 'fn': 0,
                'avg_confidence': 1.0
            }
        elif num_gt == 0:
            result = {
                'precision': 0.0, 'recall': 1.0, 'f1': 0.0,
                'tp': 0, 'fp': num_pred, 'fn': 0,
                'avg_confidence': float(np.mean(filtered_scores)) if len(filtered_scores) > 0 else 0.0
            }
        elif num_pred == 0:
            result = {
                'precision': 1.0, 'recall': 0.0, 'f1': 0.0,
                'tp': 0, 'fp': 0, 'fn': num_gt,
                'avg_confidence': 0.0
            }
        else:
            # Matching avec IoU > 0.5
            tp = 0
            matched_gt = set()
            
            for pred_box in filtered_boxes:
                best_iou = 0
                best_gt_idx = -1
                
                for gt_idx, gt_box in enumerate(gt_boxes):
                    if gt_idx not in matched_gt:
                        iou = calculate_iou(pred_box, gt_box)
                        if iou > best_iou:
                            best_iou = iou
                            best_gt_idx = gt_idx
                
                if best_iou > 0.5:
                    tp += 1
                    matched_gt.add(best_gt_idx)
            
            fp = num_pred - tp
            fn = num_gt - tp
            
            precision = tp / num_pred if num_pred > 0 else 0
            recall = tp / num_gt if num_gt > 0 else 0
            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
            
            result = {
                'precision': float(precision),
                'recall': float(recall),
                'f1': float(f1),
                'tp': int(tp),
                'fp': int(fp),
                'fn': int(fn),
                'avg_confidence': float(np.mean(filtered_scores)) if len(filtered_scores) > 0 else 0.0
            }
        
        results_by_threshold[threshold] = result
    
    # Ajouter le temps d'inf√©rence √† tous les r√©sultats
    for threshold_result in results_by_threshold.values():
        threshold_result['inference_time'] = inference_time
    
    return results_by_threshold

def run_multi_threshold_evaluation():
    """Lance l'√©valuation avec plusieurs seuils"""
    print("="*80)
    print("üéØ √âVALUATION MULTI-SEUILS - √âPOQUE 30 (SEUILS 0.5 √Ä 0.7)")
    print("="*80)
    
    # V√©rifications
    if not check_setup():
        return
    
    # Charger le mod√®le
    model, device = load_epoch30_model()
    if model is None:
        return
    
    # Charger les donn√©es
    coco, valid_images, class_ids = load_val2017_data()
    if not valid_images:
        print("‚ùå Aucune image valide trouv√©e")
        return
    
    print(f"\nüöÄ D√âMARRAGE DE L'√âVALUATION MULTI-SEUILS")
    print(f"üìä Total images √† traiter: {len(valid_images)}")
    print(f"üéØ Seuils √† tester: {CONFIDENCE_THRESHOLDS}")
    print(f"ü§ñ Mod√®le: √âpoque 30")
    
    confirmation = input(f"\n‚ö†Ô∏è Cela va traiter {len(valid_images)} images avec {len(CONFIDENCE_THRESHOLDS)} seuils. Continuer? (y/n): ")
    if confirmation.lower() != 'y':
        print("‚ùå √âvaluation annul√©e")
        return
    
    # √âvaluation compl√®te
    print(f"\n‚è∞ D√©but de l'√©valuation multi-seuils...")
    
    # Stockage des r√©sultats par seuil
    results_by_threshold = {threshold: [] for threshold in CONFIDENCE_THRESHOLDS}
    failed_images = 0
    total_inference_time = 0
    
    # Barre de progression
    pbar = tqdm(valid_images, desc="√âvaluation Multi-Seuils")
    
    for img_data in pbar:
        try:
            # √âvaluer avec TOUS les seuils en une fois
            image_results = evaluate_on_image_all_thresholds(
                model, img_data['path'], coco, img_data['id'], 
                class_ids, device
            )
            
            if image_results is not None:
                # Ajouter les r√©sultats √† chaque seuil
                for threshold, result in image_results.items():
                    results_by_threshold[threshold].append(result)
                
                # Temps d'inf√©rence (identique pour tous les seuils)
                total_inference_time += list(image_results.values())[0]['inference_time']
                
                # Mise √† jour barre de progression
                if len(results_by_threshold[0.5]) % 100 == 0:
                    # Prendre F1 du seuil 0.5 pour affichage
                    current_f1 = np.mean([r['f1'] for r in results_by_threshold[0.5]])
                    pbar.set_postfix({
                        'F1@0.5': f"{current_f1:.3f}",
                        'Images': len(results_by_threshold[0.5]),
                        '√âchecs': failed_images
                    })
            else:
                failed_images += 1
                
        except Exception as e:
            failed_images += 1
            continue
    
    pbar.close()
    
    # Calcul des r√©sultats finaux pour chaque seuil
    if not results_by_threshold[0.5]:  # V√©rifier avec le premier seuil
        print("‚ùå Aucun r√©sultat valide obtenu")
        return
    
    print(f"\nüìä CALCUL DES R√âSULTATS FINAUX POUR CHAQUE SEUIL...")
    
    final_results = {}
    
    for threshold in CONFIDENCE_THRESHOLDS:
        results = results_by_threshold[threshold]
        
        if results:
            # Moyennes pour ce seuil
            avg_precision = float(np.mean([r['precision'] for r in results]))
            avg_recall = float(np.mean([r['recall'] for r in results]))
            avg_f1 = float(np.mean([r['f1'] for r in results]))
            avg_confidence = float(np.mean([r['avg_confidence'] for r in results]))
            
            # Totaux pour ce seuil
            total_tp = int(sum([r['tp'] for r in results]))
            total_fp = int(sum([r['fp'] for r in results]))
            total_fn = int(sum([r['fn'] for r in results]))
            
            final_results[threshold] = {
                'precision': avg_precision,
                'recall': avg_recall,
                'f1': avg_f1,
                'confidence': avg_confidence,
                'tp': total_tp,
                'fp': total_fp,
                'fn': total_fn,
                'total_images': len(results),
                'failed_images': failed_images,
                'inference_time': total_inference_time / len(results)
            }
    
    # Affichage des r√©sultats comparatifs
    display_threshold_comparison_results(final_results)
    
    # Sauvegarde
    save_threshold_comparison_results(final_results, results_by_threshold)

def display_threshold_comparison_results(final_results):
    """Affiche les r√©sultats comparatifs pour tous les seuils"""
    print(f"\n{'='*100}")
    print("üéØ COMPARAISON DES SEUILS DE CONFIANCE - √âPOQUE 30")
    print(f"{'='*100}")
    
    # En-t√™te du tableau
    header = f"{'Seuil':<8} {'Pr√©cision':<12} {'Rappel':<10} {'F1-Score':<10} {'Confiance':<12} {'TP':<6} {'FP':<6} {'FN':<6}"
    print(header)
    print("-" * 100)
    
    # Afficher chaque seuil
    best_f1_threshold = None
    best_f1_score = 0
    
    for threshold in CONFIDENCE_THRESHOLDS:
        if threshold in final_results:
            result = final_results[threshold]
            
            # Trouver le meilleur F1-Score
            if result['f1'] > best_f1_score:
                best_f1_score = result['f1']
                best_f1_threshold = threshold
            
            row = (f"{threshold:<8.2f} "
                   f"{result['precision']:<12.4f} "
                   f"{result['recall']:<10.4f} "
                   f"{result['f1']:<10.4f} "
                   f"{result['confidence']:<12.4f} "
                   f"{result['tp']:<6d} "
                   f"{result['fp']:<6d} "
                   f"{result['fn']:<6d}")
            
            # Mettre en √©vidence le meilleur
            if threshold == best_f1_threshold:
                print(f"üèÜ {row}")
            else:
                print(f"   {row}")
    
    # Analyse d√©taill√©e du meilleur seuil
    if best_f1_threshold is not None:
        best_result = final_results[best_f1_threshold]
        
        print(f"\nüèÜ MEILLEUR SEUIL IDENTIFI√â: {best_f1_threshold}")
        print(f"{'='*50}")
        print(f"   üéØ F1-Score:      {best_result['f1']:.4f} ({best_result['f1']*100:.2f}%)")
        print(f"   üìä Pr√©cision:     {best_result['precision']:.4f} ({best_result['precision']*100:.2f}%)")
        print(f"   üîç Rappel:        {best_result['recall']:.4f} ({best_result['recall']*100:.2f}%)")
        print(f"   üí™ Confiance:     {best_result['confidence']:.4f} ({best_result['confidence']*100:.2f}%)")
        print(f"   ‚ö° Vitesse:       {1000/best_result['inference_time']:.1f} images/seconde")
        
        print(f"\nüìà RECOMMANDATIONS:")
        print(f"   ‚úÖ Utilisez le seuil {best_f1_threshold} pour la production")
        print(f"   üéØ Performance optimale: F1={best_result['f1']:.3f}")
        
        if best_result['precision'] > best_result['recall']:
            print(f"   üìä Profil: PR√âCISION (moins de fausses alarmes)")
        else:
            print(f"   üîç Profil: RAPPEL (d√©tection exhaustive)")
    
    # Analyse des tendances
    print(f"\nüìä ANALYSE DES TENDANCES:")
    precisions = [final_results[t]['precision'] for t in CONFIDENCE_THRESHOLDS if t in final_results]
    recalls = [final_results[t]['recall'] for t in CONFIDENCE_THRESHOLDS if t in final_results]
    f1s = [final_results[t]['f1'] for t in CONFIDENCE_THRESHOLDS if t in final_results]
    
    if len(precisions) > 1:
        if precisions[-1] > precisions[0]:
            print("   üìà Pr√©cision AUGMENTE avec le seuil (normal)")
        if recalls[-1] < recalls[0]:
            print("   üìâ Rappel DIMINUE avec le seuil (normal)")
        
        max_f1_idx = f1s.index(max(f1s))
        optimal_threshold = CONFIDENCE_THRESHOLDS[max_f1_idx]
        print(f"   üéØ √âquilibre optimal √† {optimal_threshold}")

def save_threshold_comparison_results(final_results, all_results):
    """Sauvegarde les r√©sultats de comparaison"""
    output_dir = "evaluation_results"
    os.makedirs(output_dir, exist_ok=True)
    
    # Sauvegarder r√©sum√© comparatif
    comparison_file = os.path.join(output_dir, "epoch30_threshold_comparison.json")
    with open(comparison_file, 'w') as f:
        json.dump(final_results, f, indent=2)
    
    # Sauvegarder r√©sultats d√©taill√©s par seuil
    for threshold in CONFIDENCE_THRESHOLDS:
        if threshold in all_results:
            detailed_file = os.path.join(output_dir, f"epoch30_threshold_{threshold:.2f}_detailed.json")
            with open(detailed_file, 'w') as f:
                json.dump(all_results[threshold], f, indent=2)
    
    # Cr√©er graphiques de comparaison
    create_threshold_comparison_charts(final_results)
    
    print(f"\nüíæ R√âSULTATS SAUVEGARD√âS:")
    print(f"   üìã Comparaison: {comparison_file}")
    print(f"   üìä Graphiques: {output_dir}/threshold_comparison_*.png")

def create_threshold_comparison_charts(final_results):
    """Cr√©e les graphiques de comparaison des seuils"""
    
    thresholds = sorted(final_results.keys())
    precisions = [final_results[t]['precision'] for t in thresholds]
    recalls = [final_results[t]['recall'] for t in thresholds]
    f1s = [final_results[t]['f1'] for t in thresholds]
    confidences = [final_results[t]['confidence'] for t in thresholds]
    
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
    
    # √âvolution des m√©triques
    ax1.plot(thresholds, precisions, 'b-o', label='Pr√©cision', linewidth=2, markersize=8)
    ax1.plot(thresholds, recalls, 'r-s', label='Rappel', linewidth=2, markersize=8)
    ax1.plot(thresholds, f1s, 'g-^', label='F1-Score', linewidth=2, markersize=8)
    ax1.set_xlabel('Seuil de Confiance')
    ax1.set_ylabel('Score')
    ax1.set_title('√âvolution des M√©triques par Seuil')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    ax1.set_ylim(0, 1)
    
    # Pr√©cision vs Rappel
    ax2.plot(recalls, precisions, 'ko-', linewidth=2, markersize=8)
    for i, threshold in enumerate(thresholds):
        ax2.annotate(f'{threshold}', (recalls[i], precisions[i]), 
                    xytext=(5, 5), textcoords='offset points')
    ax2.set_xlabel('Rappel')
    ax2.set_ylabel('Pr√©cision')
    ax2.set_title('Courbe Pr√©cision-Rappel')
    ax2.grid(True, alpha=0.3)
    
    # F1-Score par seuil (barres)
    bars = ax3.bar([str(t) for t in thresholds], f1s, 
                   color=['gold' if f == max(f1s) else 'skyblue' for f in f1s],
                   alpha=0.8, edgecolor='black')
    ax3.set_xlabel('Seuil de Confiance')
    ax3.set_ylabel('F1-Score')
    ax3.set_title('F1-Score par Seuil')
    ax3.grid(True, alpha=0.3)
    
    # Ajouter valeurs sur les barres
    for bar, f1 in zip(bars, f1s):
        ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,
                f'{f1:.3f}', ha='center', va='bottom', fontweight='bold')
    
    # Confiance moyenne par seuil
    ax4.plot(thresholds, confidences, 'purple', marker='D', linewidth=2, markersize=8)
    ax4.set_xlabel('Seuil de Confiance')
    ax4.set_ylabel('Confiance Moyenne')
    ax4.set_title('Confiance Moyenne par Seuil')
    ax4.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('evaluation_results/threshold_comparison_charts.png', dpi=300, bbox_inches='tight')
    plt.show()

def main():
    """Fonction principale"""
    print("üéØ √âVALUATION MULTI-SEUILS - √âPOQUE 30")
    print("="*60)
    
    print("Ce script va √©valuer le mod√®le √âpoque 30 avec plusieurs seuils:")
    print(f"Seuils test√©s: {CONFIDENCE_THRESHOLDS}")
    print("Objectif: Trouver le seuil optimal pour votre cas d'usage")
    
    choice = input("\nLancer l'√©valuation multi-seuils? (y/n): ").lower().strip()
    
    if choice == 'y':
        run_multi_threshold_evaluation()
    else:
        print("‚ùå √âvaluation annul√©e")

if __name__ == "__main__":
    main()
