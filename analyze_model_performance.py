import json
import os
import matplotlib.pyplot as plt
import numpy as np

def analyze_model_performance():
    """Analyse la performance de vos mod√®les et donne une √©valuation contextuelle"""
    
    print("="*80)
    print("üìä ANALYSE DE LA PERFORMANCE DE VOS MOD√àLES")
    print("="*80)
    
    # Charger vos r√©sultats
    results_file = 'coco_evaluation/results/complete_evaluation_results.json'
    
    if not os.path.exists(results_file):
        print("‚ùå Fichiers de r√©sultats non trouv√©s!")
        print("Lancez d'abord: python evaluate_all_models_complete.py")
        return
    
    with open(results_file, 'r', encoding='utf-8') as f:
        results = json.load(f)
    
    # Trouver le meilleur mod√®le
    best_model_name = max(results.keys(), key=lambda x: results[x]['f1'])
    best_model = results[best_model_name]
    
    print(f"üèÜ VOTRE MEILLEUR MOD√àLE: {best_model_name}")
    print("="*60)
    print(f"üìä Pr√©cision (Precision): {best_model['precision']:.1%}")
    print(f"üîç Rappel (Recall): {best_model['recall']:.1%}")
    print(f"‚≠ê F1-Score: {best_model['f1']:.1%}")
    print(f"üí™ Confiance: {best_model['confidence']:.1%}")
    
    # Benchmarks de r√©f√©rence pour la d√©tection d'objets
    benchmarks = {
        'D√©butant': {'precision': 0.30, 'recall': 0.40, 'f1': 0.35},
        'Bon': {'precision': 0.50, 'recall': 0.60, 'f1': 0.55},
        'Tr√®s bon': {'precision': 0.70, 'recall': 0.75, 'f1': 0.72},
        'Excellent': {'precision': 0.85, 'recall': 0.85, 'f1': 0.85},
        'SOTA (State-of-the-art)': {'precision': 0.90, 'recall': 0.90, 'f1': 0.90}
    }
    
    print(f"\nüìà √âVALUATION CONTEXTUELLE")
    print("="*60)
    
    # Analyser le F1-Score (m√©trique principale)
    f1_score = best_model['f1']
    
    if f1_score >= 0.85:
        level = "üèÜ EXCELLENT - Niveau professionnel"
        color = "üü¢"
    elif f1_score >= 0.72:
        level = "‚≠ê TR√àS BON - Tr√®s satisfaisant"
        color = "üü¢"
    elif f1_score >= 0.55:
        level = "üëç BON - Satisfaisant pour la plupart des applications"
        color = "üü°"
    elif f1_score >= 0.35:
        level = "üîÑ D√âBUTANT - Perfectible mais fonctionnel"
        color = "üü†"
    else:
        level = "‚ùå FAIBLE - N√©cessite des am√©liorations"
        color = "üî¥"
    
    print(f"{color} NIVEAU GLOBAL: {level}")
    print(f"üìä Votre F1-Score: {f1_score:.1%}")
    
    # Comparaison d√©taill√©e
    print(f"\nüéØ COMPARAISON AVEC LES STANDARDS")
    print("="*60)
    
    for benchmark_name, benchmark_values in benchmarks.items():
        status = "‚úÖ" if f1_score >= benchmark_values['f1'] else "‚ùå"
        print(f"{status} {benchmark_name:<25} F1: {benchmark_values['f1']:.1%}")
    
    # Analyse sp√©cifique √† votre domaine
    print(f"\nüîç ANALYSE SP√âCIFIQUE - D√âTECTION D'OBJETS PERDUS")
    print("="*60)
    
    precision = best_model['precision']
    recall = best_model['recall']
    
    # Analyse de la pr√©cision
    if precision >= 0.70:
        prec_eval = "üéØ Excellente - Peu de fausses alarmes"
    elif precision >= 0.50:
        prec_eval = "üëç Bonne - Acceptable pour un syst√®me r√©el"
    elif precision >= 0.30:
        prec_eval = "‚ö†Ô∏è Moyenne - Beaucoup de fausses d√©tections"
    else:
        prec_eval = "‚ùå Faible - Trop de fausses alarmes"
    
    # Analyse du rappel
    if recall >= 0.80:
        rec_eval = "üîç Excellent - Trouve presque tous les objets"
    elif recall >= 0.60:
        rec_eval = "üëç Bon - Trouve la plupart des objets"
    elif recall >= 0.40:
        rec_eval = "‚ö†Ô∏è Moyen - Manque certains objets"
    else:
        rec_eval = "‚ùå Faible - Manque beaucoup d'objets"
    
    print(f"Pr√©cision: {precision:.1%} ‚Üí {prec_eval}")
    print(f"Rappel: {recall:.1%} ‚Üí {rec_eval}")
    
    # Recommandations sp√©cifiques
    print(f"\nüí° RECOMMANDATIONS")
    print("="*60)
    
    if precision < 0.50:
        print("üîß Am√©liorer la pr√©cision:")
        print("   ‚Ä¢ Augmenter le seuil de confiance")
        print("   ‚Ä¢ Plus d'exemples n√©gatifs dans l'entra√Ænement")
        print("   ‚Ä¢ Fine-tuning avec des donn√©es plus propres")
    
    if recall < 0.60:
        print("üîß Am√©liorer le rappel:")
        print("   ‚Ä¢ Diminuer le seuil de confiance")
        print("   ‚Ä¢ Augmentation de donn√©es (data augmentation)")
        print("   ‚Ä¢ Plus d'exemples positifs vari√©s")
    
    if f1_score >= 0.45:
        print("‚úÖ Votre mod√®le est utilisable en production!")
        print("   ‚Ä¢ Performance acceptable pour un syst√®me r√©el")
        print("   ‚Ä¢ Peut √™tre d√©ploy√© avec monitoring")
    
    # Comparaison avec mod√®les c√©l√®bres
    print(f"\nüåü COMPARAISON AVEC MOD√àLES C√âL√àBRES")
    print("="*60)
    
    famous_models = {
        "YOLO v3 (2018)": 0.51,
        "SSD MobileNet": 0.48,
        "Faster R-CNN": 0.55,
        "YOLO v5 (2020)": 0.65,
        "YOLO v8 (2023)": 0.75
    }
    
    your_rank = 1
    for model_name, model_f1 in famous_models.items():
        if f1_score < model_f1:
            your_rank += 1
        
        comparison = "üü¢" if f1_score >= model_f1 else "üî¥"
        print(f"{comparison} {model_name:<20} F1: {model_f1:.1%}")
    
    print(f"\nüèÖ VOTRE CLASSEMENT: {your_rank}/{len(famous_models)+1}")
    
    if your_rank <= 3:
        print("üéâ Excellent! Vous rivalisez avec les meilleurs mod√®les!")
    elif your_rank <= 5:
        print("üëç Tr√®s bien! Performance comparable aux mod√®les standards")
    else:
        print("üîÑ Perfectible, mais c'est un bon d√©but!")
    
    # Visualisation
    create_performance_chart(best_model, benchmarks, famous_models)
    
    # R√©sum√© final
    print(f"\nüéØ R√âSUM√â FINAL")
    print("="*60)
    
    if f1_score >= 0.50:
        final_verdict = "üü¢ MOD√àLE PR√äT POUR LA PRODUCTION"
        advice = "Votre mod√®le a une performance suffisante pour √™tre d√©ploy√© dans un syst√®me r√©el d'objets perdus."
    elif f1_score >= 0.35:
        final_verdict = "üü° MOD√àLE FONCTIONNEL MAIS PERFECTIBLE"
        advice = "Votre mod√®le fonctionne mais pourrait b√©n√©ficier d'am√©liorations avant d√©ploiement."
    else:
        final_verdict = "üî¥ MOD√àLE √Ä AM√âLIORER"
        advice = "Le mod√®le n√©cessite plus d'entra√Ænement ou d'optimisation avant utilisation."
    
    print(f"Verdict: {final_verdict}")
    print(f"Conseil: {advice}")
    
    return best_model, f1_score

def create_performance_chart(best_model, benchmarks, famous_models):
    """Cr√©e un graphique de performance"""
    
    # Donn√©es pour le graphique
    categories = ['Pr√©cision', 'Rappel', 'F1-Score']
    your_scores = [
        best_model['precision'],
        best_model['recall'], 
        best_model['f1']
    ]
    
    good_benchmark = [
        benchmarks['Bon']['precision'],
        benchmarks['Bon']['recall'],
        benchmarks['Bon']['f1']
    ]
    
    excellent_benchmark = [
        benchmarks['Excellent']['precision'],
        benchmarks['Excellent']['recall'],
        benchmarks['Excellent']['f1']
    ]
    
    # Cr√©er le graphique
    x = np.arange(len(categories))
    width = 0.25
    
    fig, ax = plt.subplots(figsize=(10, 6))
    
    bars1 = ax.bar(x - width, your_scores, width, label='Votre mod√®le', color='#2E86AB', alpha=0.8)
    bars2 = ax.bar(x, good_benchmark, width, label='Niveau "Bon"', color='#A23B72', alpha=0.6)
    bars3 = ax.bar(x + width, excellent_benchmark, width, label='Niveau "Excellent"', color='#F18F01', alpha=0.6)
    
    # Personnalisation
    ax.set_ylabel('Score')
    ax.set_title('Comparaison de Performance de Votre Mod√®le')
    ax.set_xticks(x)
    ax.set_xticklabels(categories)
    ax.legend()
    ax.set_ylim(0, 1)
    
    # Ajouter les valeurs sur les barres
    def add_value_labels(bars):
        for bar in bars:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                   f'{height:.1%}', ha='center', va='bottom', fontsize=9)
    
    add_value_labels(bars1)
    add_value_labels(bars2)
    add_value_labels(bars3)
    
    plt.grid(axis='y', alpha=0.3)
    plt.tight_layout()
    
    # Sauvegarder
    os.makedirs('performance_analysis', exist_ok=True)
    plt.savefig('performance_analysis/model_performance_comparison.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    print("üìä Graphique sauvegard√©: performance_analysis/model_performance_comparison.png")

def main():
    best_model, f1_score = analyze_model_performance()
    
    print(f"\nüí¨ EN R√âSUM√â:")
    print("="*40)
    
    if f1_score >= 0.50:
        print("üéâ F√©licitations! Votre mod√®le a une BONNE performance!")
        print("   Vous pouvez √™tre fier de ce r√©sultat.")
    elif f1_score >= 0.35:
        print("üëç Votre mod√®le a une performance CORRECTE!")
        print("   C'est un bon d√©but, avec du potentiel d'am√©lioration.")
    else:
        print("üîÑ Votre mod√®le a besoin d'am√©liorations.")
        print("   Mais c'est normal pour un premier mod√®le!")
    
    print(f"\nüéØ Pour la d√©tection d'objets perdus, un F1-Score de {f1_score:.1%} est:")
    
    if f1_score >= 0.45:
        print("‚úÖ SUFFISANT pour un d√©ploiement en conditions r√©elles")
    elif f1_score >= 0.30:
        print("‚ö†Ô∏è UTILISABLE avec supervision humaine")
    else:
        print("‚ùå INSUFFISANT pour une utilisation pratique")

if __name__ == "__main__":
    main()